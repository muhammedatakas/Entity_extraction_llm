{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Clean Your Text Data & Classify Them with LLMs (Together AI Edition)\n",
    "This notebook demonstrates how to:\n",
    "• Standardize food descriptions (removing abbreviations, normalizing text)\n",
    "• Extract structured entities (Brand, Category, Ingredients, etc.)\n",
    "• Perform advanced classification with Together AI’s LLM chat completions\n",
    "• Handle missing data, deduplicate information, and validate results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Setup & Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from together import Together\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 20\n",
    "MAX_TOKENS = 8024\n",
    "\n",
    "# Initialize Together AI client\n",
    "try:\n",
    "    together_client = Together(api_key=\"YOUR_TOGETHER_API_KEY\")\n",
    "    print(\"Together client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to initialize Together client: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Text Preprocessing\n",
    "\n",
    "def standardize_abbreviations(text):\n",
    "    \"\"\"Standardize abbreviations in food descriptions.\"\"\"\n",
    "    abbreviation_map = {\n",
    "        # Product Types\n",
    "        'BEVE': 'BEVERAGE',\n",
    "        'VEG': 'VEGETABLE',\n",
    "        'FR': 'FRESH',\n",
    "        'PREP': 'PREPARED',\n",
    "        'BRST': 'BREAST',\n",
    "        'WHL': 'WHOLE',\n",
    "        'W/': 'WITH',\n",
    "        'W/O': 'WITHOUT',\n",
    "        \n",
    "        # States & Forms\n",
    "        'FRZ': 'FROZEN',\n",
    "        'CKD': 'COOKED',\n",
    "        'UCKD': 'UNCOOKED',\n",
    "        'BKD': 'BAKED',\n",
    "        # ...rest of the abbreviation map...\n",
    "    }\n",
    "    text_up = text.upper()\n",
    "    for abbr, full in abbreviation_map.items():\n",
    "        text_up = re.sub(fr'\\b{abbr}\\b', full, text_up)\n",
    "    return text_up.strip()\n",
    "\n",
    "def preprocess_descriptions(input_file, output_file):\n",
    "    \"\"\"Apply text standardization to each row and save output.\"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['Shrt_Desc'] = df['Shrt_Desc'].apply(standardize_abbreviations)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prompt Definition & Rate-Limited Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Prompt Definition & Rate-Limited Processing\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a specialized food information extraction system. Follow these classification rules:\n",
    "\n",
    "1. STATES vs PREPARATION:\n",
    "- States: RAW, FROZEN, DRIED, POWDER, LIQUID, SOLID\n",
    "- Preparation: BAKED, FRIED, COOKED, GRILLED, BROILED\n",
    "\n",
    "2. CATEGORY:\n",
    "- Use base categories: MEAT, DAIRY, BEVERAGES, DESSERTS, etc.\n",
    "\n",
    "3. SUB-CATEGORY:\n",
    "- Use simple product type (e.g., \"Steak\" not \"Wagyu Beef Steak\").\n",
    "\n",
    "4. INGREDIENTS:\n",
    "- Only list base ingredients explicitly mentioned.\n",
    "\n",
    "5. OUTPUT FORMAT:\n",
    "{\n",
    "    \"Brand\": \"...\",\n",
    "    \"Category\": \"...\",\n",
    "    \"Sub-Category\": \"...\",\n",
    "    \"Ingredients\": [...],\n",
    "    \"Preparation Method\": [...],\n",
    "    \"Cultural Origin\": \"...\",\n",
    "    \"State\": \"...\",\n",
    "    \"Additional Features\": [...]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "CALLS_PER_MINUTE = 6000\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS_PER_MINUTE, period=60)\n",
    "def process_with_rate_limit(batch_data, client, prompt):\n",
    "    \"\"\"Use chat completions under rate limits.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Return a JSON array. Analyze:\\n{chr(10).join(batch_data[0])}\"}\n",
    "    ]\n",
    "    return client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Batch Processing & Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Batch Processing & Flattening\n",
    "\n",
    "def process_batch(batch_data, ndb_nos):\n",
    "    \"\"\"Send batch to LLM and return parsed JSON results.\"\"\"\n",
    "    try:\n",
    "        response = process_with_rate_limit(batch_data, together_client, system_prompt)\n",
    "        if response and hasattr(response, 'choices'):\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            results = json.loads(content)\n",
    "            for item, ndb in zip(results, ndb_nos):\n",
    "                item['NDB_No'] = ndb\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Batch error: {str(e)}\")\n",
    "    return []\n",
    "\n",
    "def flatten_results(results):\n",
    "    \"\"\"Convert LLM JSON output to a row-based format.\"\"\"\n",
    "    flattened = []\n",
    "    for r in results:\n",
    "        flattened.append({\n",
    "            'NDB_No': r.get('NDB_No'),\n",
    "            'Brand': r.get('Brand'),\n",
    "            'Category': r.get('Category'),\n",
    "            'Sub_Category': r.get('Sub-Category'),\n",
    "            'Ingredients': ','.join(r.get('Ingredients', [])),\n",
    "            'Preparation_Method': ','.join(r.get('Preparation Method', [])),\n",
    "            'Cultural_Origin': r.get('Cultural Origin'),\n",
    "            'State': r.get('State'),\n",
    "            'Additional_Features': ','.join(r.get('Additional Features', []))\n",
    "        })\n",
    "    return flattened\n",
    "\n",
    "def create_fixed_batches(descriptions, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Create fixed-size batches for stable LLM calls.\"\"\"\n",
    "    return [(descriptions[i:i + batch_size], i // batch_size)\n",
    "            for i in range(0, len(descriptions), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. End-to-End Chunked Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) End-to-End Chunked Processing\n",
    "\n",
    "def process_bulk_in_chunks(input_file, output_dir, rows_per_chunk=500):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(input_file)\n",
    "    descriptions = df[\"Shrt_Desc\"].tolist()\n",
    "    ndb_nos = df[\"NDB_No\"].tolist()\n",
    "\n",
    "    total_rows = len(df)\n",
    "    chunk_index = 0\n",
    "\n",
    "    for start_idx in range(0, total_rows, rows_per_chunk):\n",
    "        end_idx = start_idx + rows_per_chunk\n",
    "        chunk_desc = descriptions[start_idx:end_idx]\n",
    "        chunk_ndb = ndb_nos[start_idx:end_idx]\n",
    "\n",
    "        batches = create_fixed_batches(chunk_desc, BATCH_SIZE)\n",
    "        all_results = []\n",
    "\n",
    "        for batch_data, batch_idx in tqdm(batches, desc=f\"Processing chunk {chunk_index}\"):\n",
    "            batch_start = batch_idx * BATCH_SIZE\n",
    "            batch_end = batch_start + BATCH_SIZE\n",
    "            ndb_subset = chunk_ndb[batch_start:batch_end]\n",
    "\n",
    "            batch_results = process_batch((batch_data, batch_idx), ndb_subset)\n",
    "            all_results.extend(flatten_results(batch_results))\n",
    "\n",
    "        chunk_file = os.path.join(output_dir, f\"output_chunk_{chunk_index}.csv\")\n",
    "        if all_results:\n",
    "            pd.DataFrame(all_results).to_csv(chunk_file, index=False)\n",
    "            print(f\"[Chunk {chunk_index}] Saved {len(all_results)} rows to {chunk_file}\")\n",
    "        else:\n",
    "            print(f\"[Chunk {chunk_index}] No results.\")\n",
    "\n",
    "        chunk_index += 1\n",
    "\n",
    "    print(\"All chunks processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Main Execution\n",
    "\n",
    "def main():\n",
    "    # 1. Preprocess descriptions (Abbreviation Expansion)\n",
    "    input_raw = \"mnt/data/replaced_v3.csv\"\n",
    "    output_preprocessed = \"mnt/data/standardized_descriptions.csv\"\n",
    "    preprocess_descriptions(input_raw, output_preprocessed)\n",
    "\n",
    "    # 2. Perform chunked extraction & classification\n",
    "    output_dir = \"mnt/data/output_chunks\"\n",
    "    process_bulk_in_chunks(output_preprocessed, output_dir, rows_per_chunk=500)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
